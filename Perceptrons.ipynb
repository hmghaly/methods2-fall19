{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In yesterday's lecture you saw perceptron models, a type of linear classifier that does not depend on a probability distribution. In today's practicum we'll walk through a simple Python implementation. We'll begin with a binary classifier, generalize it to multinomial classification, and finally add weight averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture that a binary perceptron's parameters consist of a bias term $b$ and a weight vector $\\beta$, both real-valued. For individual parameters, we define a simple class which has a weight and an `update` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "Num = Union[int, float]\n",
    "\n",
    "\n",
    "class Weight:\n",
    "    \"\"\"Simple perceptron weight.\"\"\"\n",
    "    \n",
    "    def __init__(self, value: Num = 0):\n",
    "        self.w = value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"current state {self.__class__.__name__}({self.w})\"\n",
    "    \n",
    "    def update(self, tau: int) -> None:\n",
    "        self.w += tau\n",
    "    \n",
    "    # We'll allow this to be integral- or real-valued.\n",
    "    def value(self) -> Num:\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state Weight(0)\n",
      "current state Weight(-2)\n"
     ]
    }
   ],
   "source": [
    "w = Weight(0)\n",
    "print(w)\n",
    "w.update(+1)\n",
    "w.update(-1)\n",
    "w.update(-1)\n",
    "w.update(-1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we assume that features are strings, and define $\\beta$ as a dictionary mapping from feature strings to `Weights`. We also define two methods:\n",
    "\n",
    "* `predict` will compute $\\sigma$ then apply the decision function.\n",
    "* `update` will apply the perceptron update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "FeatureVector = List[str]\n",
    "\n",
    "\n",
    "class BinaryPerceptron:\n",
    "    \"\"\"Simple binary perceptron, part 1.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.b = Weight()\n",
    "        self.beta = collections.defaultdict(Weight)\n",
    "    \n",
    "    def predict(self, features: FeatureVector) -> bool:\n",
    "        \"\"\"Returns the prediction.\"\"\"\n",
    "        # Computes sigma.\n",
    "        score  = self.b.value()\n",
    "        for feature in features:\n",
    "            score += self.beta[feature].value()\n",
    "        # Applies the decision function.\n",
    "        return score >= 0\n",
    "    \n",
    "    def update(self, features: FeatureVector, y: bool):\n",
    "        \"\"\"Applies the update rule.\"\"\"\n",
    "        yhat = self.predict(features)\n",
    "        tau = y - yhat\n",
    "        if not tau:\n",
    "            return\n",
    "        # Actually updates.\n",
    "        self.b.update(tau)\n",
    "        for feature in features:\n",
    "            self.beta[feature].update(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use this to classify. We'll reuse the pet names example from two weeks ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/hmghaly/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "Loaded 5001 female names\n",
      "Loaded 2943 male names\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import nltk\n",
    "\n",
    "# I had to install the corpus ahead of time. But you only have to do this once.\n",
    "assert nltk.download(\"names\")\n",
    "\n",
    "female = nltk.corpus.names.words(\"female.txt\")\n",
    "print(f\"Loaded {len(female)} female names\")\n",
    "male = nltk.corpus.names.words(\"male.txt\")\n",
    "print(f\"Loaded {len(male)} male names\")\n",
    "\n",
    "# Then, we'll concatenate the data and turn it into (x, y) pairs.\n",
    "name_list = [(name, \"female\") for name in female] + [(name, \"male\") for name in male]\n",
    "\n",
    "# Then, we'll shuffle the data.\n",
    "random.seed(562)\n",
    "random.shuffle(name_list)  # This works in place.\n",
    "\n",
    "# Then, we'll split it into training and test data.\n",
    "train = name_list[:-1000]\n",
    "test = name_list[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['endswith(vowel)', 'count(d)=1', 'has(d)', 'count(e)=1', 'has(e)', 'count(i)=1', 'has(i)', 'count(o)=1', 'has(o)']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "# I have modified the feature extraction code to return strings.\n",
    "\n",
    "\n",
    "FeatureVector = List[str]\n",
    "\n",
    "\n",
    "def extract_features(name: str) -> FeatureVector:\n",
    "    \"\"\"Extracts features for a single example.\"\"\"\n",
    "    name_lowercase = name.casefold()\n",
    "    vowels = frozenset(\"aeiou\")\n",
    "    features = []\n",
    "    if name[0] in vowels:\n",
    "        features.append(\"startswith(vowel)\")\n",
    "    if name[-1] in vowels:\n",
    "        features.append(\"endswith(vowel)\")\n",
    "    for char in string.ascii_lowercase:\n",
    "        count = name.count(char)\n",
    "        if count:\n",
    "            features.append(f\"count({char})={count}\")\n",
    "            features.append(f\"has({char})\")\n",
    "    return features\n",
    "\n",
    "\n",
    "# An example feature vector.\n",
    "print(extract_features(\"Bodie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['count(a)=1', 'has(a)', 'count(d)=1', 'has(d)', 'count(e)=2', 'has(e)', 'count(l)=1', 'has(l)', 'count(n)=1', 'has(n)', 'count(r)=1', 'has(r)', 'count(x)=1', 'has(x)'], False)\n"
     ]
    }
   ],
   "source": [
    "train_set = [(extract_features(name), label == \"female\") for (name, label) in train]\n",
    "\n",
    "# An example pair.\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train. We'll do 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BinaryPerceptron()\n",
    "\n",
    "# For each of 5 epochs...\n",
    "for i in range(5):\n",
    "    # For each example in that epoch...\n",
    "    for (features, y) in train_set:\n",
    "        model.update(features, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate and compute accuracy (the percentage of correctly classified test examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t0.7270\n"
     ]
    }
   ],
   "source": [
    "test_set = [(extract_features(name), label == \"female\") for (name, label) in test]\n",
    "\n",
    "examples = len(test_set)\n",
    "correct = 0\n",
    "for (features, y) in test_set:\n",
    "    if y == model.predict(features):\n",
    "        correct +=1\n",
    "        \n",
    "print(f\"Accuracy:\\t{correct / examples:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add a `fit` method to `BinaryPerceptron` that automates the training loops. It should take a list of label/feature pairs and the number of epochs.\n",
    "* Write a function (not a method of `BinaryPerceptron`) to determine accuracy.\n",
    "* Determine accuracy (percentage of correct classifications) as a function of the number of epochs. After how many epochs does performance start declining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization to the multinomial perceptron is straightforward. Recall from the lecture that the parameters consist of a bias vector $B$ and a weight matrix $\\beta$, both real-valued. We will assume here that the labels themselves are also strings. Furthermore, whereas in the lecture we wrote a particular weight as $\\beta_{i,y'}$, here we will treat $y'$ as the outer key and $i$ as the inner key. (You'll see what I mean in a second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "\n",
    "\n",
    "class MultinomialPerceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.b = collections.defaultdict(Weight)\n",
    "        # A nested defaultdict looks a bit like this...\n",
    "        self.beta = collections.defaultdict(\n",
    "            functools.partial(\n",
    "                collections.defaultdict,\n",
    "                Weight\n",
    "            )\n",
    "        )\n",
    "        \n",
    "      \n",
    "    def predict(self, features: FeatureVector) -> str:\n",
    "        \"\"\"Computes the score for a feature bundle.\"\"\"\n",
    "        # Computes sigma.\n",
    "        scores = {yprime: weight.value() for (yprime, weight)\n",
    "                  in self.b.items()}\n",
    "        # At the beginning, it won't know about the `Y` set,\n",
    "        # but we have to return something.\n",
    "        if not scores:\n",
    "            return \"\"\n",
    "        for feature in features:\n",
    "            for (yprime, weight) in self.beta[feature].items():\n",
    "                print(yprime, weight)\n",
    "                scores[yprime][feature] += weight.value()\n",
    "        # Computes yhat using argmax on a dictionary.\n",
    "        return max(scores.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    def update(self, features: FeatureVector, y: str) -> None:\n",
    "        \"\"\"Applies the update rule.\"\"\"\n",
    "        yhat = self.predict(features)\n",
    "        if y == yhat:\n",
    "            return\n",
    "        # Actually updates.\n",
    "        self.b[y].update(+1)\n",
    "        self.b[yhat].update(-1)\n",
    "        for feature in features:\n",
    "            self.beta[y][feature].update(+1)\n",
    "            self.beta[yhat][feature].update(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply this to some interesting linguistic problem. Some debugging may be required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add weight averaging. One way to do this is to define two variants of `BinaryPerceptron` or `MultinomialPerceptron` each:\n",
    "\n",
    "* one which uses averaging weights instead of `Weight`\n",
    "* one which uses normal `Weight`s and is created by applying averaging\n",
    "\n",
    "The following is the averaging weight that described in class, except that:\n",
    "    \n",
    "* I have added a `__repr__` method.\n",
    "* I have added a method `value` to return the current weight (the one used during training).\n",
    "* I have modified `average` so that it returns a `Weight` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LazyWeight:\n",
    "    \"\"\"Lazily averaged weight.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w_c = 0\n",
    "        self.t = 0\n",
    "        self.sum_w = 0\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (f\"{self.__class__.__name__}\"\n",
    "                f\"(w_c={self.w_c}, t={self.t})\")\n",
    "    \n",
    "    def freshen(self, c: int) -> None:\n",
    "        self.sum_w += (c - self.t) * self.w_c\n",
    "        self.t = c\n",
    "        \n",
    "    def update(self, tau: int, c: int) -> None:\n",
    "        self.freshen(c)\n",
    "        self.w_c += tau\n",
    "    \n",
    "    def value(self, c: int) -> int:\n",
    "        return self.w_c\n",
    "        \n",
    "    def average(self, c: int) -> Weight:\n",
    "        self.freshen(c)\n",
    "        return Weight(self.sum_w / c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with this a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LazyWeight(w_c=0, t=250)\n",
      "Weight(0.6)\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "w = LazyWeight()\n",
    "# Some time passes...\n",
    "c += 100\n",
    "w.update(+1, c)\n",
    "# Some more time passes.\n",
    "c += 150\n",
    "w.update(-1, c)\n",
    "print(w)\n",
    "# Now we're done.\n",
    "print(w.average(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build `BinaryAveragingPerceptron` which uses `LazyWeight` (instead of `Weight`) parameters.\n",
    "* Then, define a method or function which populates a `BinaryPerceptron` with the result of averaging these weights.\n",
    "* Then repeat the evaluations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a complete Python implementation of a variety of perceptron classifiers, see [`nlup.perceptron`](https://github.com/cslu-nlp/nlup/blob/master/nlup/perceptron.py)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
